---
title: "Ch 3. Describing Data"
author: "Sonya Hua"
date: "September 1, 2017"
output: rmarkdown::github_document
---

## 3.1 Simulating Data

It's important to describe and explore any data set before moving on to more complex analysis. We will be created data to be analyzed in later parts of the chapter. The process of creating data lets us practice and deepen R skills from Ch. 2. It also lets us manipulate synthetic dat, run analyses again, and examine how the results can change. R analysts often use simulated data to prove that their methods are working as expected. 

Our 1st data set is composed of observations of total sales by week for 2 products at a chain of stores around the world (20 stores total) over 2 years, with price and promotion status. 

### 3.1.1 Store Data & Setting Up the Data Structure

```{r}
k.stores <- 20
k.weeks <- 104

# create a data frame of initially missing values to hold the data
store.df <- data.frame(matrix(NA, ncol=10, nrow=k.stores * k.weeks))
names(store.df) <- c("storeNum", "Year", "Week", "p1sales", "p2sales", 
                     "p1price", "p2price", "p1prom", "p2prom", "country")  # Assign Var Names to df
str(store.df)
dim(store.df) # get dimensions of df
```

Create 2 vectors that will represent the store number and country per observation:
```{r}
(store.num <- 101:(100+k.stores))
(store.cty <- c(rep("US",3), rep("DE",5), rep("GB", 3), rep("BR",2),
                rep("JP",4), rep("AU",1), rep("CN",2)))  # Store's country location
length(store.cty)
```
Now we replace the appropriate cols in the df with those values using `rep()` to expand the vectors to match the # of stores and weeks
```{r}
store.df$storeNum <- rep(store.num, each=k.weeks) # each = # of times each element is repeated
# This is different from "times=" which repeats the whole vector n-times
store.df$country <- rep(store.cty, each =k.weeks) 
rm(store.num, store.cty) # clean up memory

```
Do the same process for the Week and Year columns:

```{r}
store.df$Week <- rep(c(1:52),times=k.stores * 2) # Replicate 52 weeks 40 times so that there will be 2 years per store
store.df$Year <- rep(c(1,2), each=52, times=k.stores)  # Replicate Year 1 and 2-  52x per store
```
Let's check the overall data structure:
```{r}
str(store.df)
```
*Observe*

* Data values populated as expected, with proper col names
* `country` has a char type when it should be factor var since it is a categorical value
* `storeNum` has a int type when it should be factor var since we will be categorizing by store number and it is a label from something else

By converting `country` and `storeNum` to factors, R will know to tream them as categorical inputs in subsequent analyses i.e. regression models. It's best practice to set var types correctly early on as they are created to avoid errors later:
```{r}
store.df$storeNum <- factor(store.df$storeNum)
store.df$country <- factor(store.df$country)
store.df$Week <- as.integer(store.df$Week)
store.df$Year <- as.integer(store.df$Year)
str(store.df)

```
*Observe* storeNum (with 20 levels) and country (with 7 levels) has been converted to factors. 
```{r}
# check first and last rows for mistakes
head(store.df, 120)

```

```{r}
tail(store.df, 120)
```
*Obs*: The data seemed to have been inputted correctly

We can now move on to filling in the rest of the data points, namely the specific measures like sales, price, promotion (Y/N)


### 3.1.2 Simulating Measurement Data Points

We'll complete store.df with random data for *store-by_week* observations of the sales, price, and promotional status of 2 products. 

##### On Randomizing Data

It's best practice to set the random number generation **seed** to make the data replicable. When setting a seed, we draw random samples in the same sequence again and get **pseudo-random** numbers via **Pseudorandom number generators (PRNGs) using `set.seed()`.

`p1prom, p2prom`: Per observation (or week), we will set the status of whether each product was promoted (1 = Yes, 0 = No), by drawing randomly from a binomial distirbution that counts the number of "heads" in a collection of coin tosses where the coin can have a specified proportion of heads). To do this, we use `rbinom(n, size, p)` for random binomial function. For every row, we draw from this distribution with specified number of heads in a single toss `n=nrow(store.df), size=1`. 

* *Assume p1 has a `p=0.1` probability and p2 has a `p=0.2` probability of being promoted *

```{r}
store.df$p1prom <- rbinom(n=nrow(store.df), size=1, p=0.10) # product 1 is promoted 10% of time
store.df$p2prom <- rbinom(n=nrow(store.df), size=1, p=0.15) # product 2 is promoted 15% of time

```

`p1price, p2price` : Assume each product is sold at 1:5 distinct price points ranging from $2.19 to $3.19 overall. We will randomly draw a price for each week by defining a vector with the price points and using `sample(x, size, replace)` to draw from it as many times as we have rows of data `size=nrow(store.df)`. We want to sample with replacement so random prices is reflected in the data with `replace=TRUE`. 
```{r}
store.df$p1price <- sample(x=c(2.19, 2.29, 2.49, 2.79, 2.99), size=nrow(store.df), replace=TRUE)
store.df$p2price <- sample(x=c(2.29, 2.49, 2.59, 2.99, 3.19), size=nrow(store.df), replace=TRUE) # slightly more expensive that product 1

# check progress
head(store.df)
```

`p1sales, p2sales` (in Units): We can calculate sales as a relative function between *price* and *promotional status* of each. Since item sales are in unit counts, we use the [Poisson Distribution](https://www.umass.edu/wsp/resources/poisson/) to generate count data `rpois(n, lambda)` where `n=` # of draws and `lambda=` mean value of units per week. For each row `(nrow=store.df)` we draw from this random poisson count. Assume product 1 mean sales (lambda=120) is higher than product 2 (lambda=100). 

* Price effects -  often follow a logarithmic relationship vs. linear so we should scale these counts up/down according to the relative prices using `log(price)`. For price effects, we assume that sales vary inversely with prices between p1 and p2. The customer will select p1 if it's cheaper than p2. E.g. sales of product 1 go up when `log(price)` of product 1 is lower than `log(price) of product 2.

* Promo effects - Assume sales get a 30% or 40% lift when each product is promoted in store. Simply multiply promotional status x 0.3 or 0.4 respectively, then multiple sales vector by that. 

Use `floor()` function to drop fractional values and ensure integer counts for weekly unit sales. 
```{r}
# first, create default sales without promotion
tmp.sales1 <- rpois(nrow(store.df),lambda=120) # p1 mean sales is slightly higher than p2
tmp.sales2 <- rpois(nrow(store.df),lambda=100)

#second, scale counts up/down based on the RATIO OF LOG(PRICE)
tmp.sales1 <- tmp.sales1 * log(store.df$p2price) / log(store.df$p1price) # when p1 is cheaper, sales go up as part of the denominator
tmp.sales2 <- tmp.sales2 * log(store.df$p1price) / log(store.df$p2price)

# third, p1 sales get a 30% lift when promoted and p2 sales get a 40% lift when promoted
store.df$p1sales = floor(tmp.sales1 * 1 + store.df$p1prom * 0.3)
store.df$p2sales = floor(tmp.sales2 * 1 + store.df$p2prom * 0.4)

# inspect data frame and check for errors
head(store.df)


```

```{r}
# use some() to further inspect from random sampling
library("car")
some(store.df)
```

*Obs*: sales seem to have been calculated correctly based on price ratios and promotional lifts. 

### 3.2 Functions to Summarize a Variable

Obs may comprise of discrete data that occurs at specific levels or continuous data with many possible values within an interval. 

#### Discrete Variables

A basic way to describe discrete data is with frequency counts. The `table()` function will count the observed prevalence of each value that occurs in a variable. 

One of the most useful features of R is that most functions produce an object that can be stored as a var and re-used. 
The `str()` command shows us that the object produced by `table()` is a special type called table object. 

An analyst might want to know how often each product was promoted at each price point. The `table()` command produces 2-way cross tabs when a 2nd variable.

```{r}
# frequency count table using table()
table(store.df$p1price)

# store the table in a var
(p1.table <- table(store.df$p1price))
str(p1.table)
```

```{r}
table(store.df$p1price, store.df$p1prom)

```
*Obs*: Product 1 is promoted approximately 10% of the time as our data intended.

Using `plot()` we can pass the table for a quick bar plot. By default, R chooses a type of plot suitable for the object type. 

```{r}
plot(p1.table, xlab="Price", ylab="# of Products Sold")
```

We can compute the exact percentage of times p1 is on promotion at each price point if we 1) assign the table to a var and then 2) divide the 2nd col of the table by the sum of the 1st and 2nd col. 
```{r}
(p1.table2 <-table(store.df$p1price, store.df$p1prom))

# Get percentage of products promoted at each price point
p1.table2[,2] / (p1.table2[,1] + p1.table2[,2])
```
#### Continuous Variables

It's helpful to summarize continuous data in terms of its distribution, extremes, central tendency (mean, median, skewness), dispersion ( the degree to which it's concentrated or dispersed), and quantile (points at specific percentiles) measures. 

* Extremes: `min(x), max(x)`
* Central Tendency: `mean(x), median(x)`
* Dispersion: `var(x), sd(x), IQR(x), mad(x)` or [median absolute deviation](http://www.statisticshowto.com/median-absolute-deviation/) ( a robust variance estimator)
* Points: `quantile(x, probs = c(....))` or Percentiles

```{r}
min(store.df$p1sales)
max(store.df$p2sales)
mean(store.df$p1prom)
median(store.df$p2sales)
var(store.df$p1sales)
sd(store.df$p1sales)
IQR(store.df$p1sales)
mad(store.df$p1sales)
quantile(store.df$p1sales, prob=c(0.25,0.5,0.75))

# central 90% of data
quantile(store.df$p1sales, probs=c(0.05,0.95))
```
We can also use sequences to get every 10% percentile:
```{r}
quantile(store.df$p1sales, c(1:10/10))

# equivalent
quantile(store.df$p1sales, probs=seq(from=0, to =1, by = 0.1))
```

Suppose we want a summary of the sales for p1 and p2 basesd on their median and IQR. We can store this summary in a df that's easier to read. We 1) create a df shell to hold our summary statistics and then 2) populate it using above functions. We'll name our columns and rows, then fill in the cells with function sales. 

```{r}
summary.df <- data.frame(matrix(NA, nrow=2, ncol=2))
names(summary.df) <- c("Median Sales", "IQR")
rownames(summary.df) <- c("Product 1", "Product 2")
summary.df["Product 1", "Median Sales"] <- median(store.df$p1sales)
summary.df["Product 2", "Median Sales"] <- median(store.df$p2sales)
summary.df["Product 1", "IQR"] <- IQR(store.df$p1sales)
summary.df["Product 2", "IQR"] <- IQR(store.df$p2sales)

summary.df
```
*Observe*:

* Median sales are higher for product 1
* Variation in sales of product 1 is higher than product 2 (the IQR is also higher)

### 3.3 Summarizing Data Frames

3 common approaches: 
* `summary()` command for preliminary descriptive inspection of a data frame or object. As best practice, always check summary reports after importing data for a quick quality check
* `describe()` command from the `psych` package reports a variety of stats for each variable in a data set including *n, range, trimmed mean, skew, kurtoses, and standard error*
* `apply()` command runs any function we specify on each of the rows AND/OR columns of an object

####3.3.1 `summary()`

`summary()` works similarly for single vectors with a horizontal display rather than vertical. The `digits=` argument if helpful to specify significant digits regardless of absolute magnitutde or the decimal position. i.e. `digits=3` means 3 significant positions. 

```{r}
summary(store.df)
```

```{r}
summary(store.df$p1sales)
```
#### 3.3.2 `describe()`

* trimmed mean: the mean after dropping a small % of extreme values. If the trimmed mean is significantly different from the overall mean, it means outliers are skewing the mean with extreme values. 

Note below, that there is an * next to the labels for `storeNum` and `country` in the output. This is a warning that they are factors and these summary statistics many not make sense. When data includes char strings or non-numerical data, `describe()` gives an error so selecting only numeric vars will solve the issue. 

```{r}
library(psych)
describe(store.df)
```


For example, we many only want to describe columns 2 and 4:9:
```{r}
describe(store.df[,c(2,4:9)]) # remember indexing uses brackets
```

#### 3.3.3 Best Practice Approach to Inspecting Data

1. Import your data with `read.csv()` or another appropriate function and check that the importation process gives no errors.

2. Convert it to a dat frame if needed `(my.data <- data.frame(DATA))` and set column names if needed `names(my.data) <- c(...)`

3. Examine `dim()` to check that the data farme has the expected number of rows and columns

4. Use `head()` and `tail()` to check the first few and last few rows for errors. Use `some()` from the `car` package to examine a few sets of random rows. 

5. Check the data frame structure with `str()` to ensure that variable types and values are appropriate. If not, change the type of vars especially for `factor` variables

6. Run `summary()` and look for any unexpected values, espeically `min/max` 

7. Use `describe()` from psych library. Reconfirm the obs counts and check trimmed mean/skew (if relevant). 

#### 3.3.4 `apply()`

`apply(x=DATA, MARGIN=MARGIN, FUN=FUNCTION)` runs any function that we specify on either each row (margin = 1) or column (margin ==2). FYI, all caps for MARGIN and FUN

For example, suppose we want to find the mean of every column of store.df except for store.df$Store which is a factor. We can `apply()` the `mean()` function to the column margin of the data.
```{r}
apply(store.df[,2:9], MARGIN=2, FUN=mean)
```

We can even use lambda functions within `apply()`. For example, we may want to find the difference between mean and meidan of each variable to flag skew within the data. 

```{r}

apply(store.df[,2:9], MARGIN=2, FUN = function(x){mean(x) - median(x)})
```
*Observe* : sales has a larger mean than a median which means the data is somewhat right skewed with a longer right tail in its distribution. This analysis shows that p1 sales is roughly larger than the median by 1 sale per week while the p2 sales is roughly larger than the median by 2 sales per week. There are some weeks with very high sales that pull the mean up. 

There are specialized versions of `apply()` that work similarly with lists and other objects such as `lapply()` and `tapply()`.

```{r}
# Previous example with apply used instead

mysummary2.df <- data.frame(matrix(NA, nrow=2, ncol=2)) # Must specify matrix to add r x c matrix
names(mysummary2.df) <- c("Median Sales", "IQR")
rownames(mysummary2.df) <- names(store.df)[4:5]
mysummary2.df[, "Median Sales"] <- apply(store.df[,c("p1sales", "p2sales")], 2, median)
mysummary2.df[, "IQR"] <- apply(store.df[,c("p1sales", "p2sales")], MARGIN=2, FUN=IQR)
mysummary2.df
```

### 3.4 Single Variable Visualization

R has many options for graphics dedicated to plotting such as `ggplot2` and `lattice` package, and specialized plots optimized for particular data such as correlation analysis.

4 things to keep in mind about graphics in R:

* R graphics are produced through commands that often seem tedious and requires trial+error
* Always use a text editor or save the code when working on plot commands as they grow large in code size and we may wight to try slight variants and to copy and paste them for reuse
* Despite the tediousness, R graphics can be very high quality
* Once we have code for useful graphics, we can reuse it with modifications. Remember to always check the chart titles, x-axis labels, and y-axis labels when reusing code.

####3.4.1 Historgrams using `hist()`

An important plot for a single continuous var is the histogram, which can be produced with the `hist()` function. Some graphic options:

* `main=` sets the main title
* `xlab=` sets the x-axis label
* `ylab=` set the y-axis label
* `breaks=NUM` sets the number of bins or breaks in the plot
* `col=` color of the bars. See [`colors()`](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) command for a list of built-in color names
* `freq=F` uses relative frequencies (or density estimates) instead of counts on the y-axis
* `xaxt="n"` X axis test is set to "none"/ removes your axis

```{r}
hist(store.df$p1sales)
```
```{r}
# add labels and title
hist(store.df$p1sales,
     main = "Product 1 Weekly Sales Frequencies, All Stores",
     xlab="Product 1 Sales (in Units Per Week)",
     ylab="Count")
```
```{r}
# add more granularity in the data as the bins are too wide. Also color the bars
hist(store.df$p1sales,
     main = "Product 1 Weekly Sales Frequencies, All Stores",
     xlab="Product 1 Sales (in Units Per Week)",
     ylab="Count",
     breaks=30,
     col="thistle")
```
*Observe* 

* the y-axis value for the height of the bars changes according to count. The count depends on the number of bins and sample size. We can make it absolute by using relative frequencies or density estimates instead of counts for each point *This makes the y-axis comparable across different sized samples by standarding the density estimates from 0 to 1*

* the plot has oddly centered labels for the X-axis. Let's remove the axis in order to replace it with a most informative axis. First remove the axis using `xaxt="n"` then call on the `axis()` function 

* `axis()` options: `side=1` alters the x-axis, `side=2` alters y-axis, `at=` tells us where to place our labels, `label=` specificies what labels to put

We can even add a density function to help us see the distribution and skew more clearly using `density()` to estimate desnity values and adding those to the `lines()` command which adds elements to the current plot in the same way we saw above for `axis` command.

* `density()`: `bw=` adjust sthe smoothing
* `line()`: `lwd=` line width
```{r}
hist(store.df$p1sales,
     main = "Product 1 Weekly Sales Frequencies, All Stores",
     xlab="Product 1 Sales (in Units Per Week)",
     ylab="Count",
     breaks=30,
     col="thistle",
     freq=F, # for density estimates
     xaxt="n") # To remove x-axis

axis(side=1, at=seq(60,300, by=20))

lines(density(store.df$p1sales, bw=9),
      type="l", col="darkred", lwd=2) # type = l for lines
```

#### 3.4.2 Boxplots

Boxplots are compact ways to visualize a distribution. `boxplot()` command is straightforward. We can add lables and use `horizontal = T` to rotate boxplots 90 degrees so it's easier to read. 

* median is the center line
* box represents the IQR from 25th to 75th percentile
* the outer lines or whiskers are extreme values that are <= 1.5x the width of the box away from the points
* Invidiaul circles are outliers

Boxplots are even more useful when comparing distributions by some other factor. Using a `~` tilde, we can specify a response variable and explanatory variable. For example,

* How do different stores compare on sales of product 2?
* How do P2 sales differ in relation to in-store promotion?
* How do P2 sales differ by price? 

We can also use `axis()` to replace the x or y axis with one that is more informative. 


```{r}
# p2 sales
boxplot(store.df$p2sales, main = "Weekly sales of P2, All stores", xlab= "Weekly Sales", ylab="P2",
         horizontal=T)
```
##### How do different stores compare on sales of product 2?
```{r}

# copy previous code and modify

boxplot(p2sales ~ storeNum, data= store.df, main = "Weekly sales of P2, All stores", xlab= "Weekly Sales", 
        ylab="Store Number", horizontal=T, col="lightblue")
```
##### How do P2 sales differ in relation to in-store promotion?
```{r}

```

```{r}
# add our own axis to Yes No
boxplot(p2sales ~ p2prom, data= store.df, main = "Weekly sales of P2, All stores", xlab= "Weekly Sales", 
        ylab="Promotion (Y/N)", horizontal=T, col="azure", yaxt="n")
# Modify y-axis
axis(side=2, at=c(1,2), labels=c("No","Yes")) # at location starts from the bottom to top


```
##### How do P2 sales differ by price? 
```{r}
boxplot(p2sales ~ p2price, data= store.df, main = "Weekly sales of P2 by Price Point", xlab= "Weekly Sales", 
        ylab="Price Point ($)", horizontal=T, col="lightyellow")
```

#### QQ Plots to Check Normality

QQ plots (Quantile-quantile) are a quick way to check one's dat aagainst a distribution that we think it should come from. For example, in order to interpret correlation coefficients *r*, it needs to be interpreted under an assumption that data are normally distributed. A QQ plot can confirm that the distribution is normal by *plotting observed quantiles of the data against the quantiles that would be expected from a normal distribution*.

`qqnorm()` command compares data vs. normal distribution. 
`qqline()` adds a diagonal line for easier reading.

There's common patterns that appear in QQ plots and knowing [how to interpret them](http://emp.byui.edu/BrownD/Stats-intro/dscrptv/graphs/qq-plot_egs.htm) is worthwile. 

```{r}
qqnorm(store.df$p1sales)
qqline(store.df$p1sales)
```
*Observe* The tails of the distribution bow away from the line, showing the distribution is skewed and there are heavier tails possibly due to outliers

If we are using models or stat functions that assume normally distributed data, we might want to transform our data then check if the transformation provides a normal distribution by QQ plotting it agian after transformation. A common pattern in marketing data is a logarithmic distribution and using a `log()` transformation may help with bring it to a more normal distribution. 
```{r}
qqnorm(log(store.df$p1sales))
qqline(log(store.df$p1sales))
```
*Observe* After log transformation, the sales figures are much better aligned to a normal distribution although there is still some skew

#### 3.4.4 Cumulative Distribution 

Another useful univariate plot is the **[ECDF](http://docs.battlemesh.org/v8/ecdf.html)** or empirical cumulative distribution function. ECDF's show the cumulative proportion of data values in our sample. It's an easy way to inspect a distribution and read off percentile values. 

`plot()` can only make a few plot types on its own and otherwise need to be given an object that includes more info such as X and Y values. Many R functions produce objects automatically that are suitable as input for `plot()`. 

We'll plot the ECDF of p1 sales thru a few steps:

1) Use `ecdf()` to find the ecdf of hte data
2) Wrap `plot()` around that adding options such as titles
3) Modify labels on the axis such as relabeling hte proportions as percentiles. The `paste()` function combines a number vector with the "%" symbol to make each label. 
4) Add lines at specific percentiles using the `abline()` function to add vertical and horizontal lines at desired percentiles. We don't have to tell R the exact value at which to point the line. Instead we use `quantile( , pr=)` to find it. 

ECDF's are useful for highling discontinuities in the data, long tails, and specific points of interest. 

```{r}
plot(ecdf(store.df$p1sales),
     main="Cumulative distrib. of P1 Weekly Sales",
     ylab="Cumulative Proportion",
     xlab=c("P1 weekly sales, all stores", "90% of weeks sold <= 171 units"),
     yaxt="n")

axis(side=2, at=seq(0,1,by=0.1), las=1, labels=paste(seq(0,1,by=0.1),"%", sep=""))

abline(h=0.9, lty=3) # for horizontal line; "lty=3" for dotted line type
abline(v=quantile(store.df$p1sales, pr=0.9), lty=3)  # find 90% percentile of p1sales using quantile()
```

#### 3.4.5 Language Brief: by() and aggregate()

How can we break out data by factors and summarize it similar to cross-tabs or pivot tables? We can summarize by a factor within the data itself using `by()` and `aggregate()`. 

`by(data=DATA, INDICES=INDICES, FUN=FUNCTION)`: `INDICES` indicates the grouping factor to divide data by. Each factor applies the function FUN to its group. A limitation of `by()` is that the result is easy to read but not structured for reuse. To save results as data to use for other purposes such as plotting, consider using `aggregate()`

`aggregate(x=DATA, by=BY, FUN=FUNCTION)`: applies a particular function (FUN) according to division of the data specified by a factor 
`(by)`. This provides a data frame which we can save for reuse. 

For example, we want to find the average sales of P1 by store:
```{r}
# Using positional arguments
head(by(store.df$p1sales, store.df$storeNum, mean))

aggregate(store.df$p1sales, list(store.df$storeNum), mean)  # BY= must be a list

```

```{r}
# To group it by more than one factor, use a list() of factors

by(store.df$p1sales, INDICES=list(store.df$storeNum, store.df$Year), mean)
```

